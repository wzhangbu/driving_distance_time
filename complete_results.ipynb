{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the libiaries and the fire stations and property data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/DRIVING_DISTANCE\n",
      "There are 6389671 data in the file.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import read_data\n",
    "import functions\n",
    "import os\n",
    "import re\n",
    "import glob as glob\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "states = ['NY']\n",
    "#  CT, MA, NH, NJ, NY, PA\n",
    "\n",
    "\n",
    "\n",
    "FS_path = \"s3://pr-home-datascience/DSwarehouse/Datasources/FireStationLocation/FireStation/FireStations_0.csv\"\n",
    "# Use read_data.py to load the data\n",
    "for state in states:\n",
    "    property_path = \"s3://pr-home-datascience/Users/test/IntermediateDrive/Quantarium/AD/good_address/rundt=202504/\" + 'state='+states[0] + '/'\n",
    "    df_FS = read_data.ReadData(FS_path, state = state, directory=False)\n",
    "\n",
    "    df_property = read_data.ReadData(property_path, state=state, directory=True)\n",
    "    print(f'There are {len(df_property.data)} data in the file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the missing data\n",
    "The big chunk request may have some missing data. Please check the files with \"fail\" keywords and complete them with the following function.\n",
    "\n",
    "The default directory of chunks with missing data is './missing_data/' + state \\\n",
    "You can also use the Amazon S3 directory:\\\n",
    "missing_data_path = 's3://pr-home-datascience/Projects/AdHoc/InternProjects/2025/2025InternSummer Driving distance for Prospect Table/'+state+'/'\n",
    "\n",
    "\n",
    "The default directory of chunks with complete data after reruning the request is './complete_data/' + state\\\n",
    "Or use the Amazon S3 directory:\\\n",
    "output_path = 's3://pr-home-datascience/Projects/AdHoc/InternProjects/2025/2025InternSummer Driving distance for Prospect Table/'+state+'/complete_data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\\\n",
    "1: Set the current state to process\\\n",
    "2: Define paths for missing data and output directory\\\n",
    "3: Retrieve all CSV files in the missing data folder\\\n",
    "4: Define chunksize per state (default to 300k for most states)\\\n",
    "5: Get total number of properties for this state\\\n",
    "6: Loop through each missing file to recompute using complete_osrm_results\\\n",
    "7: Rerun OSRM routing on the failed batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./missing_data/NY/FS_batch_0_n300000_failed.csv', './missing_data/NY/FS_batch_19_n300000_failed.csv', './missing_data/NY/FS_batch_1_n300000_failed.csv', './missing_data/NY/FS_batch_20_n300000_failed.csv', './missing_data/NY/FS_batch_21_n300000_failed.csv', './missing_data/NY/FS_batch_2_n300000_failed.csv']\n",
      "Currently handling the file  ./missing_data/NY/FS_batch_19_n300000_failed.csv\n",
      "[✓] Loading input data...\n",
      "NaN in df_result: 0\n",
      "Indices with NaN travel_time_min: []\n",
      "Fill the NaN with 0.0.\n",
      "[!] Missing OSRM results for 700 rows. Starting retry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:26<00:00, 10.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging the final data has length of 300000.\n",
      "[✓] Done! Final results with 300000 rows saved to ./complete_data/NY/FS_batch_19_n300000.csv.\n"
     ]
    }
   ],
   "source": [
    "# We are grouping the data by states\n",
    "state = 'NY'\n",
    "\n",
    "# the paths for missing data and for the results\n",
    "missing_data_path = os.path.join('.', 'missing_data', state)\n",
    "\n",
    "# output path to save the data\n",
    "output_path = os.path.join('.', 'complete_data', state)\n",
    "\n",
    "# obtain all the data file, format: \"FS_batch_?_n\"+chunksize +'failed.csv'\n",
    "csv_files = glob.glob(os.path.join(missing_data_path, '*.csv'))\n",
    "csv_files.sort()\n",
    "\n",
    "# adjust chunksize based on the chunksize from property_FS.ipynb\n",
    "chunksizes = {'MA': 100000, 'NJ': 200000, 'NY': 300000, 'PA': 300000, 'CT': 300000, 'NH': 300000}\n",
    "# the length of total data\n",
    "total = len(df_property.data)\n",
    "\n",
    "# print(csv_files)\n",
    "for file in csv_files:\n",
    "# for file in csv_files[7:]:\n",
    "    print('Currently handling the file ', file)\n",
    "    chunksize = chunksizes[state]\n",
    "    # get the batch index of the file, the index will be used to locate the property information.\n",
    "    match = re.search(r'FS_batch_(\\d+)_', file) \n",
    "    index = int(match.group(1))  \n",
    "\n",
    "    # The chunk that needs to be filled\n",
    "    start = index * chunksize\n",
    "    end = min((index + 1) * chunksize, total)\n",
    "    df_chunk = df_property.data.iloc[start:end]\n",
    "\n",
    "    tmp = functions.complete_osrm_results(\n",
    "        df_chunk,\n",
    "        df_FS.data,\n",
    "        file,\n",
    "        output_path=output_path + f\"/FS_batch_{index}_n{chunksize}.csv\",\n",
    "        batch_size=50,\n",
    "        osrm_url = \"http://router.project-osrm.org\"\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the data\n",
    "Combine all the completed chunks, we need to verify the sequence of the property is the same as the original data, and merge them into one files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\\\n",
    "1: Set the target state, load property data from S3 using read_data.py\\\n",
    "2: Define the output path for merged result\\\n",
    "3: Define a helper function to extract batch index from filenames\\\n",
    "4: Load all completed batch CSVs\\\n",
    "5: Get chunk size and total row count\\\n",
    "6: Optional - check for NaNs in travel_time_min in each batch file\\\n",
    "7: Merge all batches, verifying QPID alignment\\\n",
    "8: Check that the order of QPIDs matches the original dataset\\\n",
    "9: Concatenate all batches into a single DataFrame and add 'state' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_0_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_1_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_2_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_3_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_4_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_5_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_6_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_7_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_8_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_9_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_10_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_11_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_12_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_13_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_14_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_15_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_16_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_17_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_18_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_19_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_20_n300000.csv\n",
      "Number of NaN values in travel_time_min: 0 in file ./complete_data/NY/FS_batch_21_n300000.csv\n",
      "File 0 qpid index match passed.\n",
      "File 1 qpid index match passed.\n",
      "File 2 qpid index match passed.\n",
      "File 3 qpid index match passed.\n",
      "File 4 qpid index match passed.\n",
      "File 5 qpid index match passed.\n",
      "File 6 qpid index match passed.\n",
      "File 7 qpid index match passed.\n",
      "File 8 qpid index match passed.\n",
      "File 9 qpid index match passed.\n",
      "File 10 qpid index match passed.\n",
      "File 11 qpid index match passed.\n",
      "File 12 qpid index match passed.\n",
      "File 13 qpid index match passed.\n",
      "File 14 qpid index match passed.\n",
      "File 15 qpid index match passed.\n",
      "File 16 qpid index match passed.\n",
      "File 17 qpid index match passed.\n",
      "File 18 qpid index match passed.\n",
      "File 19 qpid index match passed.\n",
      "File 20 qpid index match passed.\n",
      "File 21 qpid index match passed.\n",
      "Final merged file saved to: s3://pr-home-datascience/Projects/AdHoc/InternProjects/2025/2025InternSummer Driving distance for Prospect Table/results/\n"
     ]
    }
   ],
   "source": [
    "state = 'NY'\n",
    "# Use read_data.py to load the data\n",
    "property_path = \"s3://pr-home-datascience/Users/test/IntermediateDrive/Quantarium/AD/good_address/rundt=202504/\" + 'state='+state + '/'\n",
    "df_property = read_data.ReadData(property_path, state=state, directory=True)\n",
    "\n",
    "# output directory\n",
    "output_path = 's3://pr-home-datascience/Projects/AdHoc/InternProjects/2025/2025InternSummer Driving distance for Prospect Table/results/'\n",
    "\n",
    "# sort the results in the order of the batch index\n",
    "def extract_batch_number(path):\n",
    "    filename = os.path.basename(path)\n",
    "    match = re.search(r'FS_batch_(\\d+)_', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# The complete data directory\n",
    "# The default is './complet_data/' + state + '/'\n",
    "data_path = './complete_data/' + state\n",
    "csv_files = glob.glob(os.path.join(data_path, '*.csv'))\n",
    "csv_files.sort(key=extract_batch_number)\n",
    "\n",
    "chunksizes = {'MA': 100000, 'NJ': 200000, 'NY': 300000, 'PA': 300000, 'CT': 300000, 'NH': 300000}\n",
    "chunksize = chunksizes[state]\n",
    "\n",
    "# the length of total data\n",
    "total = len(df_property.data)\n",
    "\n",
    "# to record the results after merging\n",
    "all_batches = []\n",
    "\n",
    "# check the NaN in each chunk\n",
    "for file in csv_files:\n",
    "    batch_index = extract_batch_number(file)\n",
    "    df_batch = pd.read_csv(file)\n",
    "    num_nan = df_batch[\"travel_time_min\"].isna().sum()\n",
    "    print(f\"Number of NaN values in travel_time_min: {num_nan} in file\", file)\n",
    "\n",
    "# merging the data\n",
    "for file in csv_files:\n",
    "    # load chunks\n",
    "    batch_index = extract_batch_number(file)\n",
    "    df_batch = pd.read_csv(file)\n",
    "    start = batch_index * chunksize\n",
    "    end = min((batch_index + 1) * chunksize, total)\n",
    "\n",
    "    # fill the NaN with 0.0 use if necessary\n",
    "    # df_batch['travel_time_min'] = df_batch['travel_time_min'].fillna(0.0)\n",
    "    # df_batch['travel_dist_mile'] = df_batch['travel_dist_mile'].fillna(0.0)\n",
    "\n",
    "    ## The following is to verify the qpid are in the same order as the original property dataframe\n",
    "    expected_qpids = df_property.data.iloc[start:end]['QPID'].reset_index(drop=True)\n",
    "    batch_qpids = df_batch['qpid'].reset_index(drop=True)\n",
    "\n",
    "    # The results shouls have the exact same qpid order with the original data\n",
    "    # If failed, print out key messages\n",
    "    if not batch_qpids.equals(expected_qpids):\n",
    "        print(f\"[ERROR] qpid mismatch in file: {file}\")\n",
    "        mismatch_mask = batch_qpids != expected_qpids\n",
    "        mismatch_indices = mismatch_mask[mismatch_mask].index.tolist()\n",
    "        print(f\"Total mismatches: {len(mismatch_indices)}\")\n",
    "\n",
    "        # print the first 10 mismatches\n",
    "        for i in mismatch_indices[:10]:\n",
    "            print(f\"  Index {i}: batch_qpid = {batch_qpids.iloc[i]}, expected_qpid = {expected_qpids.iloc[i]}\")\n",
    "    else:\n",
    "        print(f\"File {batch_index} qpid index match passed.\")\n",
    "\n",
    "    all_batches.append(df_batch)\n",
    "\n",
    "\n",
    "# merge all data\n",
    "df_merged = pd.concat(all_batches, ignore_index=True)\n",
    "df_merged['state'] = state\n",
    "\n",
    "# save to the file\n",
    "df_merged.to_csv(output_path + state+'_FS_merged_all.csv', index=False)\n",
    "print(f\"Final merged file saved to: {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract results examples\n",
    "We will extract some examples from the estimations and the property information and verify them online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       QPID State  PA_Latitude  PA_Longitude  Match_Code Location_Code  \\\n",
      "0  40136573    MA      42.4751      -70.9358           1           AS0   \n",
      "1  42438550    MA      42.1538      -71.6540           1           AS0   \n",
      "2  42438555    MA      42.1536      -71.6536           1           AS0   \n",
      "3  64805738    MA      41.6612      -70.4364           1          AP02   \n",
      "4  64805739    MA      41.7000      -70.2793           1           AS0   \n",
      "\n",
      "  add_check geo_check  source   rundt state  \n",
      "0      good         Y       2  202504    MA  \n",
      "1      good         Y       2  202504    MA  \n",
      "2      good         Y       2  202504    MA  \n",
      "3      good         Y       2  202504    MA  \n",
      "4      good         Y       2  202504    MA  \n",
      "None\n",
      "-----------seperation--------------------\n",
      "       qpid  property_lat  property_lon  fire_station_id  station_lat  \\\n",
      "0  40136573       42.4751      -70.9358         10218765    42.479782   \n",
      "1  42438550       42.1538      -71.6540         10229068    42.110870   \n",
      "2  42438555       42.1536      -71.6536         10229068    42.110870   \n",
      "3  64805738       41.6612      -70.4364         10229269    41.657401   \n",
      "4  64805739       41.7000      -70.2793         10491479    41.700834   \n",
      "\n",
      "   station_lon  travel_time_min  travel_dist_mile state  \n",
      "0   -70.934619         1.753333          0.501447    MA  \n",
      "1   -71.672249         9.108333          4.779898    MA  \n",
      "2   -71.672249         9.076667          4.761692    MA  \n",
      "3   -70.410483         4.391667          1.662230    MA  \n",
      "4   -70.302758         2.898333          1.317183    MA  \n",
      "-----------seperation--------------------\n"
     ]
    }
   ],
   "source": [
    "state = 'MA'\n",
    "\n",
    "# Load the property information in state\n",
    "property_path = \"s3://pr-home-datascience/Users/test/IntermediateDrive/Quantarium/AD/good_address/rundt=202504/\" + 'state='+state + '/'\n",
    "df_property = read_data.ReadData(property_path, state=state, directory=True)\n",
    "print(df_property.preview())\n",
    "\n",
    "print('-----------seperation--------------------')\n",
    "# Load the estimation from property to firestation in state\n",
    "output_path = 's3://pr-home-datascience/Projects/AdHoc/InternProjects/2025/2025InternSummer Driving distance for Prospect Table/results/'\n",
    "df_estimation = pd.read_csv(output_path+state+'_FS_merged_all.csv',  sep = ',', low_memory=False)\n",
    "print(df_estimation.head())\n",
    "\n",
    "print('-----------seperation--------------------')\n",
    "# Load the fire station in state\n",
    "FS_path = \"s3://pr-home-datascience/DSwarehouse/Datasources/FireStationLocation/FireStation/FireStations_0.csv\"\n",
    "df_FS = read_data.ReadData(FS_path, state = state, directory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2090486\n",
      "3.583302136084028\n",
      "qpid                   67071259\n",
      "property_lat          30.324418\n",
      "property_lon         -81.664297\n",
      "fire_station_id        10229509\n",
      "station_lat           42.110551\n",
      "station_lon           -73.35453\n",
      "travel_time_min        1224.945\n",
      "travel_dist_mile    1047.842102\n",
      "state                        MA\n",
      "Name: 2090486, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(df_estimation['travel_time_min']))\n",
    "print(np.mean(df_estimation['travel_time_min']))\n",
    "print(df_estimation.iloc[2090486])\n",
    "\n",
    "\n",
    "\n",
    "# print(len(df_FS.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driving_distance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
